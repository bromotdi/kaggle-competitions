{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf9a17d",
   "metadata": {
    "papermill": {
     "duration": 0.006654,
     "end_time": "2024-02-21T18:23:16.046707",
     "exception": false,
     "start_time": "2024-02-21T18:23:16.040053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import numpy , pandas and OS Library, default code snippet given by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6429c1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:16.062109Z",
     "iopub.status.busy": "2024-02-21T18:23:16.061378Z",
     "iopub.status.idle": "2024-02-21T18:23:16.797108Z",
     "shell.execute_reply": "2024-02-21T18:23:16.796249Z"
    },
    "papermill": {
     "duration": 0.746042,
     "end_time": "2024-02-21T18:23:16.799459",
     "exception": false,
     "start_time": "2024-02-21T18:23:16.053417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma/keras/gemma_2b_en/1/config.json\n",
      "/kaggle/input/gemma/keras/gemma_2b_en/1/tokenizer.json\n",
      "/kaggle/input/gemma/keras/gemma_2b_en/1/metadata.json\n",
      "/kaggle/input/gemma/keras/gemma_2b_en/1/model.weights.h5\n",
      "/kaggle/input/gemma/keras/gemma_2b_en/1/assets/tokenizer/vocabulary.spm\n",
      "/kaggle/input/gemma/pytorch/2b/1/config.json\n",
      "/kaggle/input/gemma/pytorch/2b/1/gemma-2b.ckpt\n",
      "/kaggle/input/gemma/pytorch/2b/1/tokenizer.model\n",
      "/kaggle/input/python-codes-data-25k/python_codes_25k_train_data.json\n",
      "/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n",
      "/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n",
      "/kaggle/input/cleaned-python-25k-data/extracted_data.json\n",
      "/kaggle/input/parquetfile-python-25k/0000.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ba40a",
   "metadata": {
    "papermill": {
     "duration": 0.006904,
     "end_time": "2024-02-21T18:23:16.813641",
     "exception": false,
     "start_time": "2024-02-21T18:23:16.806737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install Keras-NLP and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2427f4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:16.829306Z",
     "iopub.status.busy": "2024-02-21T18:23:16.828427Z",
     "iopub.status.idle": "2024-02-21T18:23:45.343258Z",
     "shell.execute_reply": "2024-02-21T18:23:45.342020Z"
    },
    "papermill": {
     "duration": 28.525231,
     "end_time": "2024-02-21T18:23:45.345719",
     "exception": false,
     "start_time": "2024-02-21T18:23:16.820488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\r\n",
      "tensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886d7ec",
   "metadata": {
    "papermill": {
     "duration": 0.007256,
     "end_time": "2024-02-21T18:23:45.360461",
     "exception": false,
     "start_time": "2024-02-21T18:23:45.353205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### One can also try calling pretrained models from Hf, but will need latest transformers library for tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831c972d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:45.376724Z",
     "iopub.status.busy": "2024-02-21T18:23:45.376404Z",
     "iopub.status.idle": "2024-02-21T18:23:45.380832Z",
     "shell.execute_reply": "2024-02-21T18:23:45.379959Z"
    },
    "papermill": {
     "duration": 0.015194,
     "end_time": "2024-02-21T18:23:45.382815",
     "exception": false,
     "start_time": "2024-02-21T18:23:45.367621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.38.0\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "#Another notebook another day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2c973",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2024-02-21T18:23:45.397335",
     "exception": false,
     "start_time": "2024-02-21T18:23:45.389901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Go to hf and search for flytech/python-codes-25k and download parquet file and upload the dataset on kaggle and call it by pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faad24fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:45.414203Z",
     "iopub.status.busy": "2024-02-21T18:23:45.413891Z",
     "iopub.status.idle": "2024-02-21T18:23:46.033314Z",
     "shell.execute_reply": "2024-02-21T18:23:46.032509Z"
    },
    "papermill": {
     "duration": 0.631144,
     "end_time": "2024-02-21T18:23:46.035716",
     "exception": false,
     "start_time": "2024-02-21T18:23:45.404572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_parquet(\"/kaggle/input/parquetfile-python-25k/0000.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9904e84",
   "metadata": {
    "papermill": {
     "duration": 0.006782,
     "end_time": "2024-02-21T18:23:46.049735",
     "exception": false,
     "start_time": "2024-02-21T18:23:46.042953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### We will pass parquet file by extracting Instruction and output column and will convert it into List + to save memory will delete the file variable later, also will only take rows from 0 to 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ec7bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:46.065871Z",
     "iopub.status.busy": "2024-02-21T18:23:46.064814Z",
     "iopub.status.idle": "2024-02-21T18:23:47.296913Z",
     "shell.execute_reply": "2024-02-21T18:23:47.296081Z"
    },
    "papermill": {
     "duration": 1.242402,
     "end_time": "2024-02-21T18:23:47.299223",
     "exception": false,
     "start_time": "2024-02-21T18:23:46.056821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=file.apply(lambda row:f\"Instruction:\\n{row.instruction}\\n\\nResponse:\\n{row.output}\",axis=1).values.tolist()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7c1ce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:47.315394Z",
     "iopub.status.busy": "2024-02-21T18:23:47.314638Z",
     "iopub.status.idle": "2024-02-21T18:23:47.325189Z",
     "shell.execute_reply": "2024-02-21T18:23:47.324316Z"
    },
    "papermill": {
     "duration": 0.020527,
     "end_time": "2024-02-21T18:23:47.327067",
     "exception": false,
     "start_time": "2024-02-21T18:23:47.306540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9110bd5",
   "metadata": {
    "papermill": {
     "duration": 0.006915,
     "end_time": "2024-02-21T18:23:47.341300",
     "exception": false,
     "start_time": "2024-02-21T18:23:47.334385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### You can see by print statement what our \"data\" variable contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17334e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:47.356799Z",
     "iopub.status.busy": "2024-02-21T18:23:47.356225Z",
     "iopub.status.idle": "2024-02-21T18:23:47.361031Z",
     "shell.execute_reply": "2024-02-21T18:23:47.360161Z"
    },
    "papermill": {
     "duration": 0.014959,
     "end_time": "2024-02-21T18:23:47.363198",
     "exception": false,
     "start_time": "2024-02-21T18:23:47.348239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Create a shopping list based on my inputs!\n",
      "\n",
      "Response:\n",
      "```python\n",
      "shopping_list = {}\n",
      "while True:\n",
      "    item = input('Enter an item or type 'done' to finish: ')\n",
      "    if item == 'done': break\n",
      "    quantity = input(f'Enter the quantity for {item}: ')\n",
      "    shopping_list[item] = quantity\n",
      "print(f'Your shopping list: {shopping_list}')\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad91354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:47.379057Z",
     "iopub.status.busy": "2024-02-21T18:23:47.378771Z",
     "iopub.status.idle": "2024-02-21T18:23:59.895362Z",
     "shell.execute_reply": "2024-02-21T18:23:59.894509Z"
    },
    "papermill": {
     "duration": 12.527279,
     "end_time": "2024-02-21T18:23:59.897676",
     "exception": false,
     "start_time": "2024-02-21T18:23:47.370397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 18:23:49.165705: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 18:23:49.165806: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 18:23:49.288029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b9213",
   "metadata": {
    "papermill": {
     "duration": 0.007058,
     "end_time": "2024-02-21T18:23:59.912187",
     "exception": false,
     "start_time": "2024-02-21T18:23:59.905129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import necessary backend and to avoid memory fragmentation use the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e295f564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:59.928220Z",
     "iopub.status.busy": "2024-02-21T18:23:59.927717Z",
     "iopub.status.idle": "2024-02-21T18:23:59.932180Z",
     "shell.execute_reply": "2024-02-21T18:23:59.931343Z"
    },
    "papermill": {
     "duration": 0.014592,
     "end_time": "2024-02-21T18:23:59.934050",
     "exception": false,
     "start_time": "2024-02-21T18:23:59.919458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" #or torch or tensorflow\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748ff35",
   "metadata": {
    "papermill": {
     "duration": 0.006999,
     "end_time": "2024-02-21T18:23:59.948415",
     "exception": false,
     "start_time": "2024-02-21T18:23:59.941416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Call the Gemma model by adding model from right hand side by add model button and add gemma 2billion 94.3 GB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fcc4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:23:59.963843Z",
     "iopub.status.busy": "2024-02-21T18:23:59.963568Z",
     "iopub.status.idle": "2024-02-21T18:25:04.851717Z",
     "shell.execute_reply": "2024-02-21T18:25:04.850886Z"
    },
    "papermill": {
     "duration": 64.898456,
     "end_time": "2024-02-21T18:25:04.854039",
     "exception": false,
     "start_time": "2024-02-21T18:23:59.955583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/1' to your Kaggle notebook...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0addcb83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:25:04.871301Z",
     "iopub.status.busy": "2024-02-21T18:25:04.870746Z",
     "iopub.status.idle": "2024-02-21T18:25:04.902936Z",
     "shell.execute_reply": "2024-02-21T18:25:04.902081Z"
    },
    "papermill": {
     "duration": 0.042797,
     "end_time": "2024-02-21T18:25:04.904790",
     "exception": false,
     "start_time": "2024-02-21T18:25:04.861993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07b88d",
   "metadata": {
    "papermill": {
     "duration": 0.008479,
     "end_time": "2024-02-21T18:25:04.922080",
     "exception": false,
     "start_time": "2024-02-21T18:25:04.913601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Use Lora to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2926dec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:25:04.941453Z",
     "iopub.status.busy": "2024-02-21T18:25:04.940911Z",
     "iopub.status.idle": "2024-02-21T18:25:05.061407Z",
     "shell.execute_reply": "2024-02-21T18:25:05.060412Z"
    },
    "papermill": {
     "duration": 0.132696,
     "end_time": "2024-02-21T18:25:05.063696",
     "exception": false,
     "start_time": "2024-02-21T18:25:04.931000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0daa803",
   "metadata": {
    "papermill": {
     "duration": 0.008475,
     "end_time": "2024-02-21T18:25:05.081353",
     "exception": false,
     "start_time": "2024-02-21T18:25:05.072878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### To save memory decrease the context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f744199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:25:05.100339Z",
     "iopub.status.busy": "2024-02-21T18:25:05.099735Z",
     "iopub.status.idle": "2024-02-21T18:29:51.358981Z",
     "shell.execute_reply": "2024-02-21T18:29:51.357993Z"
    },
    "papermill": {
     "duration": 286.405199,
     "end_time": "2024-02-21T18:29:51.495251",
     "exception": false,
     "start_time": "2024-02-21T18:25:05.090052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708539946.840267      86 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1708539946.917229      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 206ms/step - loss: 1.0880 - sparse_categorical_accuracy: 0.6695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fbbbc4e7ca0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 128\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35185fd",
   "metadata": {
    "papermill": {
     "duration": 0.087383,
     "end_time": "2024-02-21T18:29:51.670490",
     "exception": false,
     "start_time": "2024-02-21T18:29:51.583107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save the model, such that after inferencing, you can call the finetuned model and again finetune on more number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d03170d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:29:51.846967Z",
     "iopub.status.busy": "2024-02-21T18:29:51.846131Z",
     "iopub.status.idle": "2024-02-21T18:30:48.170132Z",
     "shell.execute_reply": "2024-02-21T18:30:48.168824Z"
    },
    "papermill": {
     "duration": 56.415156,
     "end_time": "2024-02-21T18:30:48.172787",
     "exception": false,
     "start_time": "2024-02-21T18:29:51.757631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm.save(\"version_finetuned.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78c034",
   "metadata": {
    "papermill": {
     "duration": 0.250029,
     "end_time": "2024-02-21T18:30:57.887498",
     "exception": false,
     "start_time": "2024-02-21T18:30:57.637469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### You can see it answers the question brilliantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025fea58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T18:30:58.103432Z",
     "iopub.status.busy": "2024-02-21T18:30:58.102591Z",
     "iopub.status.idle": "2024-02-21T18:31:22.686147Z",
     "shell.execute_reply": "2024-02-21T18:31:22.684969Z"
    },
    "papermill": {
     "duration": 24.679037,
     "end_time": "2024-02-21T18:31:22.688626",
     "exception": false,
     "start_time": "2024-02-21T18:30:58.009589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1708540280.859736      26 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "write a code for creating a list in python\n",
      "\n",
      "Response:\n",
      "# Create a list of strings.\n",
      "list_of_strings = [‘apple’, ‘banana’, ‘cherry’, ‘orange’,]\n",
      "print(list_of_strings)\n",
      "\n",
      "Explanation:\n",
      "In this code, first we create a list of strings.\n",
      "Then we print the list of strings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction=\"write a code for creating a list in python\"\n",
    "response=\"\"\n",
    "prompt = f\"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "print(gemma_lm.generate(prompt, max_length=128))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4475088,
     "sourceId": 7672184,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4475204,
     "sourceId": 7672359,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4475281,
     "sourceId": 7672449,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 10260,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 5305,
     "sourceId": 11216,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 492.419953,
   "end_time": "2024-02-21T18:31:25.706667",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T18:23:13.286714",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

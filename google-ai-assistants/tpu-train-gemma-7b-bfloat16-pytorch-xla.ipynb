{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781c7ef1",
   "metadata": {
    "papermill": {
     "duration": 0.005154,
     "end_time": "2024-02-24T05:33:53.153065",
     "exception": false,
     "start_time": "2024-02-24T05:33:53.147911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d18a06",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-02-24T05:33:53.165096Z",
     "iopub.status.busy": "2024-02-24T05:33:53.164842Z",
     "iopub.status.idle": "2024-02-24T05:35:11.186077Z",
     "shell.execute_reply": "2024-02-24T05:35:11.184980Z"
    },
    "papermill": {
     "duration": 78.029839,
     "end_time": "2024-02-24T05:35:11.188397",
     "exception": false,
     "start_time": "2024-02-24T05:33:53.158558",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Install package for inferences\n",
    "# !pip install -qq --no-deps /kaggle/input/daigt-pip/peft-0.6.0-py3-none-any.whl\n",
    "# !pip install -qq --no-deps /kaggle/input/daigt-pip/transformers-4.35.0-py3-none-any.whl\n",
    "# !pip install -qq --no-deps /kaggle/input/daigt-pip/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# !pip install -qq --no-deps /kaggle/input/daigt-pip/optimum-1.14.0-py3-none-any.whl\n",
    "!pip install -qq git+https://github.com/huggingface/transformers.git  -U \n",
    "!pip install -qq git+https://github.com/huggingface/accelerate.git  -U \n",
    "!pip install -qq git+https://github.com/huggingface/optimum.git  -U \n",
    "!pip install -qq bitsandbytes \n",
    "!pip install -qq git+https://github.com/huggingface/peft.git  -U \n",
    "\n",
    "# !pip install -q /kaggle/input/autocorrect/autocorrect-2.6.1.tar\n",
    "# # Eanble 4-bit CUDA functions for PyTorch\n",
    "# #!pip install -q bitsandbytes --no-index --find-link /kaggle/input/llm-detect-pip/bitsandbytes-0.41.1-py3-none-any.whl\n",
    "# #!pip install -q accelerate --no-index --find-link /kaggle/input/llm-detect-pip/accelerate-0.24.1-py3-none-any.whl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f31670",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-02-24T05:35:11.201430Z",
     "iopub.status.busy": "2024-02-24T05:35:11.200996Z",
     "iopub.status.idle": "2024-02-24T05:35:44.887203Z",
     "shell.execute_reply": "2024-02-24T05:35:44.886058Z"
    },
    "papermill": {
     "duration": 33.695074,
     "end_time": "2024-02-24T05:35:44.889510",
     "exception": false,
     "start_time": "2024-02-24T05:35:11.194436",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install packaages for training on TPUs (notebook internet must enable)\n",
    "!pip install -qq sentencepiece==0.1.99 \n",
    "!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n",
    "!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n",
    "!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n",
    "# !cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8a372",
   "metadata": {
    "papermill": {
     "duration": 0.00571,
     "end_time": "2024-02-24T05:35:44.900867",
     "exception": false,
     "start_time": "2024-02-24T05:35:44.895157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports for training on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce365c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:35:44.913780Z",
     "iopub.status.busy": "2024-02-24T05:35:44.913487Z",
     "iopub.status.idle": "2024-02-24T05:36:11.911694Z",
     "shell.execute_reply": "2024-02-24T05:36:11.910871Z"
    },
    "papermill": {
     "duration": 27.007781,
     "end_time": "2024-02-24T05:36:11.914223",
     "exception": false,
     "start_time": "2024-02-24T05:35:44.906442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Imports for TPU XLA \n",
    "import torch_xla.debug.profiler as xp\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import torch_xla.experimental.xla_sharding as xs\n",
    "import torch_xla.runtime as xr\n",
    "xr.use_spmd() # To enable PyTorch/XLA SPMD execution mode for automatic parallelization\n",
    "\n",
    "# \"experimental\" XLA packages\n",
    "import torch_xla.experimental.xla_sharding as xs \n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "# from spmd_util import partition_module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d61ac",
   "metadata": {
    "papermill": {
     "duration": 0.00551,
     "end_time": "2024-02-24T05:36:11.926173",
     "exception": false,
     "start_time": "2024-02-24T05:36:11.920663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports for inference and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949515fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:11.939090Z",
     "iopub.status.busy": "2024-02-24T05:36:11.938646Z",
     "iopub.status.idle": "2024-02-24T05:36:21.918556Z",
     "shell.execute_reply": "2024-02-24T05:36:21.917465Z"
    },
    "papermill": {
     "duration": 9.988587,
     "end_time": "2024-02-24T05:36:21.920518",
     "exception": false,
     "start_time": "2024-02-24T05:36:11.931931",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, sklearn, os, gc, re, random, time, sys\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import (\n",
    "    AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig,\n",
    "    LlamaConfig, GemmaConfig, AutoModel, AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",
    ") \n",
    "\n",
    "from accelerate import cpu_offload, dispatch_model\n",
    "from accelerate.utils.modeling import infer_auto_device_map\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = 99\n",
    "\n",
    "print(f'Torch Version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a602700",
   "metadata": {
    "papermill": {
     "duration": 0.005868,
     "end_time": "2024-02-24T05:36:21.932306",
     "exception": false,
     "start_time": "2024-02-24T05:36:21.926438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2256f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:21.945061Z",
     "iopub.status.busy": "2024-02-24T05:36:21.944660Z",
     "iopub.status.idle": "2024-02-24T05:36:21.948640Z",
     "shell.execute_reply": "2024-02-24T05:36:21.947815Z"
    },
    "papermill": {
     "duration": 0.012689,
     "end_time": "2024-02-24T05:36:21.950390",
     "exception": false,
     "start_time": "2024-02-24T05:36:21.937701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_FOLD = 5\n",
    "SEED = 42\n",
    "DEBUG = False  # True: trained LLM on small samples for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451ce812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:21.962778Z",
     "iopub.status.busy": "2024-02-24T05:36:21.962554Z",
     "iopub.status.idle": "2024-02-24T05:36:21.970065Z",
     "shell.execute_reply": "2024-02-24T05:36:21.969342Z"
    },
    "papermill": {
     "duration": 0.016183,
     "end_time": "2024-02-24T05:36:21.971925",
     "exception": false,
     "start_time": "2024-02-24T05:36:21.955742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6974e8",
   "metadata": {
    "papermill": {
     "duration": 0.005674,
     "end_time": "2024-02-24T05:36:21.982928",
     "exception": false,
     "start_time": "2024-02-24T05:36:21.977254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11e73b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:21.995228Z",
     "iopub.status.busy": "2024-02-24T05:36:21.994992Z",
     "iopub.status.idle": "2024-02-24T05:36:22.002828Z",
     "shell.execute_reply": "2024-02-24T05:36:22.002138Z"
    },
    "papermill": {
     "duration": 0.016021,
     "end_time": "2024-02-24T05:36:22.004407",
     "exception": false,
     "start_time": "2024-02-24T05:36:21.988386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "def cv_split(train_data):\n",
    "    skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n",
    "    X = train_data.loc[:, train_data.columns != \"label\"]\n",
    "    y = train_data.loc[:, train_data.columns == \"label\"]\n",
    "    # Split the train into 5 folds\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(X, X['source'])):\n",
    "        train_data.loc[valid_index, \"fold\"] = fold\n",
    "\n",
    "    print(train_data.groupby(\"fold\")[\"label\"].value_counts())\n",
    "    return train_data\n",
    "\n",
    "def load_train_data():\n",
    "    train_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\", sep=',')\n",
    "    train_prompts_df = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\", sep=',')\n",
    "\n",
    "    # rename column generated to label and remove used 'id' and 'prompt_id' columns\n",
    "    # Label: 1 indicates generated texts (by LLMs) \n",
    "    train_df['source'] = \"train_essay\"\n",
    "    train_df = train_df.rename(columns={'generated': 'label'})\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    train_df = train_df.drop(['id', 'prompt_id'], axis=1)\n",
    "    \n",
    "    # Include external data\n",
    "    external_df = pd.read_csv(\"/kaggle/input/daigt-v4-train-dataset/train_v4_drcat_01.csv\", sep=',')\n",
    "    # RDizzl3_seven = True\n",
    "    excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\n",
    "    external_df = external_df[~(external_df['prompt_name'].isin(excluded_prompt_name_list))]\n",
    "    # We only need 'text' and 'label' columns\n",
    "    external_df = external_df[[\"text\", \"label\", \"source\"]]\n",
    "    \n",
    "    # Merge train and external data into train_data\n",
    "    train_data = pd.concat([train_df, external_df])\n",
    "    train_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # print(f\"Train data has shape: {train_data.shape}\")\n",
    "    print(f\"Train data {train_data.value_counts('label')}\") # 1: generated texts 0: human texts\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00b8022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:22.016799Z",
     "iopub.status.busy": "2024-02-24T05:36:22.016574Z",
     "iopub.status.idle": "2024-02-24T05:36:31.813728Z",
     "shell.execute_reply": "2024-02-24T05:36:31.812684Z"
    },
    "papermill": {
     "duration": 9.805563,
     "end_time": "2024-02-24T05:36:31.815624",
     "exception": false,
     "start_time": "2024-02-24T05:36:22.010061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label\n",
      "1    39491\n",
      "0    23212\n",
      "Name: count, dtype: int64\n",
      "fold  label\n",
      "0.0   1        7900\n",
      "      0        4641\n",
      "1.0   1        7897\n",
      "      0        4644\n",
      "2.0   1        7899\n",
      "      0        4642\n",
      "3.0   1        7898\n",
      "      0        4642\n",
      "4.0   1        7897\n",
      "      0        4643\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "      <th>aux_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, an...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seems to be cooling\" says Elisabeth rosenthal. To und...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a one or any other motor vehicle to work? The stor...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps one of the worlds greatest advancements and techno...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "0  Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and ...   \n",
       "1  Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, an...   \n",
       "2  \"America's love affair with it's vehicles seems to be cooling\" says Elisabeth rosenthal. To und...   \n",
       "3  How often do you ride in a car? Do you drive a one or any other motor vehicle to work? The stor...   \n",
       "4  Cars are a wonderful thing. They are perhaps one of the worlds greatest advancements and techno...   \n",
       "\n",
       "   label       source  fold  aux_label  \n",
       "0      0  train_essay   2.0          0  \n",
       "1      0  train_essay   2.0          0  \n",
       "2      0  train_essay   4.0          0  \n",
       "3      0  train_essay   4.0          0  \n",
       "4      0  train_essay   2.0          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = load_train_data()\n",
    "train_data = cv_split(train_data)\n",
    "\n",
    "# Unicode decode 'NFKD'\n",
    "train_data['text'] = train_data['text'].str.normalize('NFKD')\n",
    "train_data['text'] = train_data['text'].str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# remove trailling white spaces, and ;-$ []()\n",
    "train_data['text'] = train_data['text'].str.strip()\n",
    "train_data['text'] = train_data['text'].str.replace(r'[ \\t]+$', \"\", regex=True)\n",
    "train_data['text'] = train_data['text'].str.replace(r'[\\;\\-\\$]', \"\", regex=True)\n",
    "train_data['text'] = train_data['text'].str.replace(r'[\\[\\]\\(\\)]', \"\", regex=True)\n",
    "\n",
    "AUX_LABEL_MAP = dict(zip(train_data['source'].unique(),range(train_data['source'].nunique())))\n",
    "train_data['aux_label'] = train_data['source'].map(AUX_LABEL_MAP)\n",
    "\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7377b",
   "metadata": {
    "papermill": {
     "duration": 0.005803,
     "end_time": "2024-02-24T05:36:31.827868",
     "exception": false,
     "start_time": "2024-02-24T05:36:31.822065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Obfuscation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b41c28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:31.879404Z",
     "iopub.status.busy": "2024-02-24T05:36:31.879066Z",
     "iopub.status.idle": "2024-02-24T05:36:31.882821Z",
     "shell.execute_reply": "2024-02-24T05:36:31.882117Z"
    },
    "papermill": {
     "duration": 0.012391,
     "end_time": "2024-02-24T05:36:31.884390",
     "exception": false,
     "start_time": "2024-02-24T05:36:31.871999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from autocorrect import Speller\n",
    "\n",
    "# speller = Speller(lang='en', fast=True)\n",
    "# train_data[\"text\"] = train_data[\"text\"].progress_apply(speller)\n",
    "\n",
    "# display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a596af50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:31.897313Z",
     "iopub.status.busy": "2024-02-24T05:36:31.897049Z",
     "iopub.status.idle": "2024-02-24T05:36:31.900902Z",
     "shell.execute_reply": "2024-02-24T05:36:31.900147Z"
    },
    "papermill": {
     "duration": 0.012387,
     "end_time": "2024-02-24T05:36:31.902629",
     "exception": false,
     "start_time": "2024-02-24T05:36:31.890242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # https://www3.nd.edu/~busiforc/handouts/cryptography/Letter%20Frequencies.html\n",
    "# BIGRAMS = ['th', 'he', 'in', 'en', 'nt', 're', 'er', 'an', 'ti', 'es', 'on', 'at', 'se',\n",
    "#            'nd', 'or', 'ar', 'al', 'te', 'co', 'de', 'to', 'ra', 'et', 'ed']\n",
    "# TRIGRAMS = ['the', 'and', 'tha', 'ent', 'ing', 'ion', 'tio', 'for','nde', 'has', 'nce']\n",
    "\n",
    "# def replace_uni(text):\n",
    "#     bi = random.choice(BIGRAMS[:10])\n",
    "#     c1,c2 = random.sample(bi, 2)\n",
    "#     return text.replace(c1, c2)\n",
    "\n",
    "# def replace_bi(text):\n",
    "#     bi = random.choice(BIGRAMS)\n",
    "#     c = random.choice(bi)\n",
    "#     return text.replace(c, bi)\n",
    "\n",
    "# def replace_tri(text):\n",
    "#     ti = random.choice(TRIGRAMS)\n",
    "#     c = random.choice(ti)\n",
    "#     return text.replace(c, ti)\n",
    "    \n",
    "# def Obfuscate(text, P=0.3):\n",
    "#     if random.random() < P:\n",
    "#         n = random.random()\n",
    "#         if n<0.4:\n",
    "#             text = replace_uni(text)\n",
    "#         elif n<0.8:\n",
    "#             text = replace_bi(text)\n",
    "#         else:\n",
    "#             text = replace_tri(text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e058a9bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:31.915418Z",
     "iopub.status.busy": "2024-02-24T05:36:31.915190Z",
     "iopub.status.idle": "2024-02-24T05:36:31.989918Z",
     "shell.execute_reply": "2024-02-24T05:36:31.989058Z"
    },
    "papermill": {
     "duration": 0.083329,
     "end_time": "2024-02-24T05:36:31.991564",
     "exception": false,
     "start_time": "2024-02-24T05:36:31.908235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "      <th>aux_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "0  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "1  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "2  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "3  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "4  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "\n",
       "   label       source  fold  aux_label  \n",
       "0      0  train_essay   2.0          0  \n",
       "1      0  train_essay   2.0          0  \n",
       "2      0  train_essay   4.0          0  \n",
       "3      0  train_essay   4.0          0  \n",
       "4      0  train_essay   2.0          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data.loc[train_data['source']!='train_essay', 'text'] = train_data.loc[train_data['source']!='train_essay', 'text'].progress_apply(Obfuscate)\n",
    "train_data[\"text\"] = \"Check whether the following essay is human written or AI generated? Remember, the text might have undergone multiple stages of obfuscation.\\n\" + train_data[\"text\"]\n",
    "\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e093006b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.004984Z",
     "iopub.status.busy": "2024-02-24T05:36:32.004752Z",
     "iopub.status.idle": "2024-02-24T05:36:32.008273Z",
     "shell.execute_reply": "2024-02-24T05:36:32.007572Z"
    },
    "papermill": {
     "duration": 0.012635,
     "end_time": "2024-02-24T05:36:32.010125",
     "exception": false,
     "start_time": "2024-02-24T05:36:31.997490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from autocorrect import Speller\n",
    "# from collections import Counter\n",
    "\n",
    "# def cnt_diffs(text1, text2):\n",
    "#     cnt1 = Counter(text1)\n",
    "#     cnt2 = Counter(text2)\n",
    "#     c1 = (cnt1 - cnt2).most_common(1)\n",
    "#     c2 = (cnt2 - cnt1).most_common(1)\n",
    "#     c1 = c1[0][1] if len(c1) else 0\n",
    "#     c2 = c2[0][1] if len(c2) else 0\n",
    "#     return max(c1, c2)\n",
    "\n",
    "# speller = Speller(lang='en', fast=True)\n",
    "# speller_precise = Speller(lang='en', fast=False)\n",
    "# train_data[\"text_spell\"] = train_data[\"text\"].progress_apply(speller)\n",
    "\n",
    "# DIFF_TH = 30\n",
    "# txt_diff = train_data.apply(lambda r: cnt_diffs(r.text, r.text_spell), axis=1)\n",
    "# train_data.loc[txt_diff >= DIFF_TH, 'text'] = train_data.loc[txt_diff >= DIFF_TH, 'text'].progress_apply(speller_precise)\n",
    "\n",
    "# display(train_data.head())\n",
    "# display(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f47b2",
   "metadata": {
    "papermill": {
     "duration": 0.006006,
     "end_time": "2024-02-24T05:36:32.022015",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.016009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the LLM model on TPUs\n",
    "\n",
    "NLTK package is used to correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a325a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.035063Z",
     "iopub.status.busy": "2024-02-24T05:36:32.034842Z",
     "iopub.status.idle": "2024-02-24T05:36:32.039063Z",
     "shell.execute_reply": "2024-02-24T05:36:32.038338Z"
    },
    "papermill": {
     "duration": 0.013085,
     "end_time": "2024-02-24T05:36:32.040845",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.027760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from textblob import TextBlob\n",
    "\n",
    "def pre_processing_text(text):\n",
    "    corrected_text = text.replace('\\n', ' ')\n",
    "    #corrected_text = TextBlob(corrected_text).correct()\n",
    "    return corrected_text\n",
    "\n",
    "# def parallelize_processing_texts(texts):\n",
    "#     corrected_texts = []\n",
    "#     # Run the spelling correction in parallel with a progress bar\n",
    "# #     with tqdm(total=len(texts)) as pbar:\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = list(executor.map(pre_processing_text, texts))\n",
    "#         for future in as_completed(futures):\n",
    "#             corrected_text = future.result()\n",
    "#             corrected_texts.append(corrected_text)\n",
    "#         if len(corrected_texts) % 100 == 0:    \n",
    "#             print(f\"Complete {len(corrected_texts)} / {len(texts)}\")\n",
    "#     return corrected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d03d7191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.053957Z",
     "iopub.status.busy": "2024-02-24T05:36:32.053736Z",
     "iopub.status.idle": "2024-02-24T05:36:32.062715Z",
     "shell.execute_reply": "2024-02-24T05:36:32.062006Z"
    },
    "papermill": {
     "duration": 0.017663,
     "end_time": "2024-02-24T05:36:32.064284",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.046621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class GemmaForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, TARGET_MODEL, DEVICE, model_name='gemma7b'):\n",
    "        super().__init__()\n",
    "        self.DEVICE = DEVICE\n",
    "        base_model = AutoModel.from_pretrained(TARGET_MODEL, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "        # No idea why this is needed\n",
    "        base_model.config.pretraining_tp = 1 # 1 is 7b\n",
    "        # Assign Padding TOKEN\n",
    "        base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        # LoRa\n",
    "        peft_config = LoraConfig(\n",
    "            r=16,  # Use larger 'r' value increase more parameters during training\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.10,\n",
    "            bias='none',\n",
    "            inference_mode=False,\n",
    "            # Only Use Output and Values Projection\n",
    "            target_modules=[\n",
    "                'o_proj',\n",
    "                'v_proj',\n",
    "            ],\n",
    "        )\n",
    "        # Load the new PEFT model\n",
    "        self.base_model = get_peft_model(base_model, peft_config)\n",
    "        # Classification Dropout\n",
    "        self.head_drop = torch.nn.Dropout(p=0.15)\n",
    "        # Classification Heads\n",
    "        self.llm_head = torch.nn.Linear(base_model.config.hidden_size, 1, bias=False).to(self.DEVICE)\n",
    "        self.llm_head_aux = torch.nn.Linear(base_model.config.hidden_size, len(AUX_LABEL_MAP), bias=False).to(self.DEVICE)\n",
    "        # Print Model Prameters\n",
    "        self.base_model.print_trainable_parameters()\n",
    "        \n",
    "    def get_seq_lens(self, input_ids):\n",
    "        return (torch.eq(input_ids, self.base_model.config.pad_token_id).int().argmax(-1) - 1).to(self.DEVICE)\n",
    "    \n",
    "    def get_pooled_out(self, hidden_states, input_ids):\n",
    "        bs_r = torch.arange(hidden_states.shape[0], device=self.DEVICE)\n",
    "        return hidden_states[bs_r, self.get_seq_lens(input_ids)].to(self.DEVICE)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs = self.get_pooled_out(hidden_states[0].to(dtype=torch.float32), input_ids=input_ids)\n",
    "        outputs = self.head_drop(outputs)\n",
    "        return self.llm_head(outputs), self.llm_head_aux(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfbf08",
   "metadata": {
    "papermill": {
     "duration": 0.005759,
     "end_time": "2024-02-24T05:36:32.075952",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.070193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Partition Stratagy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b55c78f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.089021Z",
     "iopub.status.busy": "2024-02-24T05:36:32.088777Z",
     "iopub.status.idle": "2024-02-24T05:36:32.094093Z",
     "shell.execute_reply": "2024-02-24T05:36:32.093414Z"
    },
    "papermill": {
     "duration": 0.013787,
     "end_time": "2024-02-24T05:36:32.095770",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.081983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From this repo: https://github.com/HeegyuKim/torch-xla-SPMD\n",
    "GEMMA_RULES = (\n",
    "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
    "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
    "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
    "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
    "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
    ")\n",
    "\n",
    "ALL_RULES = [\n",
    "#     (GPTNeoXConfig, GPTNEOX_RULES),\n",
    "#     (T5Config, T5_RULES),\n",
    "#     (LlamaConfig, LLAMA_RULES),\n",
    "#     (MixtralConfig, MIXTRAL_RULES),\n",
    "    (GemmaConfig, GEMMA_RULES)\n",
    "]\n",
    "def find_rule(model):\n",
    "    for config, rule in ALL_RULES:\n",
    "        if model.config.__class__ == config:\n",
    "            return rule\n",
    "    raise Exception(\"unsupported model to partitioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea4c316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.108740Z",
     "iopub.status.busy": "2024-02-24T05:36:32.108518Z",
     "iopub.status.idle": "2024-02-24T05:36:32.114475Z",
     "shell.execute_reply": "2024-02-24T05:36:32.113806Z"
    },
    "papermill": {
     "duration": 0.014216,
     "end_time": "2024-02-24T05:36:32.115999",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.101783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "strkey2id = {\n",
    "    \"dp\": 0,\n",
    "    \"fsdp\": 1,\n",
    "    \"mp\": 2\n",
    "}\n",
    "\n",
    "def partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n",
    "    partition_specs = find_rule(model)\n",
    "    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n",
    "    # print(rule)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        module.to(device)\n",
    "        # print(name, module.__class__.__name__)\n",
    "        if isinstance(module, (nn.Embedding, nn.Linear)):\n",
    "            for rule_pattern, spec in rule:\n",
    "                if re.findall(rule_pattern, name):\n",
    "                    if verbose:\n",
    "                        print(\"match\", rule_pattern, name)\n",
    "                    \n",
    "                    xs.mark_sharding(module.weight, mesh, spec)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e2aa59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.129470Z",
     "iopub.status.busy": "2024-02-24T05:36:32.129198Z",
     "iopub.status.idle": "2024-02-24T05:36:32.166759Z",
     "shell.execute_reply": "2024-02-24T05:36:32.166058Z"
    },
    "papermill": {
     "duration": 0.046426,
     "end_time": "2024-02-24T05:36:32.168561",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.122135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "\n",
    "class TrainModelTPU():\n",
    "    def __init__(self, fold, epochs, train_data):\n",
    "        self.fold = fold\n",
    "        self.train_data = train_data\n",
    "        self.DEBUG = DEBUG\n",
    "        self.SEED = SEED\n",
    "        self.NUM_LABELS = 1 # Total Number of Labels (0:human texts, 1:LLM generated texts)\n",
    "        self.MAX_LENGTH = 1024\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.LR = 3e-4 # Learning rate\n",
    "        self.WD = 0.1 # Weight Decay Ratio\n",
    "        self.AUX_WEIGHT = 0.4 # Auxilary Loss Weight\n",
    "        self.DEVICE = xm.xla_device() # Initialize TPU Device\n",
    "        self.NUM_EPOCHS = epochs # Number Of Training Epochs\n",
    "        self.NUM_WARMUP_STEPS = 60 # Number of Warmup Steps\n",
    "        # Load the baseline model\n",
    "        self.TARGET_MODEL = \"/kaggle/input/gemma/transformers/7b/2\"\n",
    "        # Model saving path\n",
    "        self.SAVE_PATH = f'/kaggle/working/gemma7b_fold{fold}_TPU'\n",
    "        self.load_model()  # Load the pretrained LLM and tokenizer \n",
    "    \n",
    "    # Disply trainable layers of LLM\n",
    "    def display_model_layers(self):        \n",
    "        # Dispaly trainable layers for verification\n",
    "        trainable_layers = []\n",
    "        n_trainable_params = 0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            # Layer Parameter Count\n",
    "            n_params = int(torch.prod(torch.tensor(param.shape)))\n",
    "            # Only Trainable Layers\n",
    "            if param.requires_grad:\n",
    "                # Add Layer Information\n",
    "                trainable_layers.append({\n",
    "                    '#param': n_params,\n",
    "                    'name': name,\n",
    "                    'dtype': param.data.dtype,\n",
    "                    'params': param\n",
    "                })\n",
    "                n_trainable_params += n_params\n",
    "\n",
    "        display(pd.DataFrame(trainable_layers))\n",
    "        print(f\"Number of trainable parameters: {n_trainable_params:,} \"\n",
    "              f\"Number of trainable layers: {len(trainable_layers)}\")\n",
    "        \n",
    "    def unfreeze_last_nlayers(self, model, n=1):\n",
    "        num_layers = model.config.num_hidden_layers\n",
    "        unfreeze_layers = [f'layers.{i}' for i in range(num_layers-n, num_layers)]\n",
    "        for name, param in model.named_parameters():\n",
    "            for u in unfreeze_layers:\n",
    "                if u in name:\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "    # Load pretrained LLM and tokenizer\n",
    "    def load_model(self):\n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.TARGET_MODEL, use_fast=False)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = GemmaForSequenceClassification(self.tokenizer, self.TARGET_MODEL, self.DEVICE)\n",
    "        self.unfreeze_last_nlayers(self.model.base_model, n=1)\n",
    "#         # No idea why this is needed\n",
    "#         base_model.config.pretraining_tp = 1 # 1 is 7b\n",
    "#         # Assign Padding TOKEN\n",
    "#         base_model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "#         # LoRa\n",
    "#         peft_config = LoraConfig(\n",
    "#             r=16 if self.DEBUG else 64,  # Use larger 'r' value increase more parameters during training\n",
    "#             lora_alpha=16,\n",
    "#             lora_dropout=0.10,\n",
    "#             bias='none',\n",
    "#             inference_mode=False,\n",
    "#             # Only Use Output and Values Projection\n",
    "#             target_modules=[\n",
    "#                 'o_proj',\n",
    "#                 'v_proj',\n",
    "#             ],\n",
    "#         )\n",
    "#         # Create LoRa Model\n",
    "# #         if self.epoch > 1:\n",
    "# #             print(f\"Continue Training the model from {str(self.TARGET_MODEL)}\")\n",
    "# #             # Continue training PEFT model\n",
    "# #             self.model = PeftModel.from_pretrained(base_model, str(self.TARGET_MODEL))\n",
    "# #         else:\n",
    "#         # Load the new PEFT model\n",
    "#         self.model = get_peft_model(base_model, peft_config)\n",
    "#         self.head_drop = torch.nn.Dropout(p=0.1)\n",
    "#         self.llm_head = torch.nn.Linear(base_model.config.hidden_size, 1, bias=False)\n",
    "#         # Display Trainable Parameters to make sure we load the model successfully\n",
    "#         self.model.print_trainable_parameters()\n",
    "# #         if self.DEBUG:\n",
    "# #             self.display_model_layers()\n",
    "        print(\"Complete loading pretrained LLM model\")\n",
    "    \n",
    "    def create_optimizer_scheduler(self, STEPS_PER_EPOCH, NUM_EPOCHS):        \n",
    "        # Optimizer (Adam)\n",
    "        if self.WD:\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.LR, weight_decay=self.LR*self.WD)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "\n",
    "        # Cosine Learning Rate With Warmup\n",
    "        lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                                    optimizer=optimizer,\n",
    "                                    num_warmup_steps=self.NUM_WARMUP_STEPS,\n",
    "                                    num_training_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\n",
    "        # Set the data type for the optimizer's state (e.g., momentum buffers)\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n",
    "                    state[v] = v.to(dtype=torch.float32)\n",
    "        print(\"Complete creating optimizer and lr scheduler\")\n",
    "        print(\"optimizer\", optimizer)\n",
    "        print(\"lr_scheduler\", lr_scheduler)\n",
    "        return optimizer, lr_scheduler\n",
    "        \n",
    "    def partition_mesh(self):\n",
    "        # Number of TPU Nodes to ensure we can access TPUs and partition the model into mesh\n",
    "        num_devices = xr.global_runtime_device_count()\n",
    "        mesh_shape = (1, num_devices, 1)\n",
    "        device_ids = np.array(range(num_devices))\n",
    "        mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n",
    "        partition_module(self.model.base_model, mesh)\n",
    "        return num_devices, mesh\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Tokenize Data\n",
    "        tokens = self.tokenizer(text, # Texts\n",
    "                                padding='max_length', # Pad texts to maximum length\n",
    "                                max_length=self.MAX_LENGTH, # Maximum token length\n",
    "                                truncation=True, # Truncate texts if they are too long\n",
    "                                return_tensors='np', # Return Numpy array\n",
    "                                )\n",
    "        # Input IDs are the token IDs\n",
    "        INPUT_IDS = tokens['input_ids'][:,:self.MAX_LENGTH]\n",
    "        # Attention Masks to Ignore Padding Tokens\n",
    "        ATTENTION_MASKS = tokens['attention_mask'][:,:self.MAX_LENGTH]\n",
    "        return INPUT_IDS, ATTENTION_MASKS\n",
    "        \n",
    "     # Create a training dataset \n",
    "    def create_dataset(self, TEXTS, GENERATED, AUX_LABELS, N_SAMPLES, mesh):\n",
    "        IDXS = np.arange(N_SAMPLES-(N_SAMPLES%self.BATCH_SIZE))\n",
    "        while True:\n",
    "            # Shuffle Indices\n",
    "            np.random.shuffle(IDXS)\n",
    "            # Iterate Over All Indices Once\n",
    "            for idxs in IDXS.reshape(-1, self.BATCH_SIZE):\n",
    "                # Tokenize\n",
    "                input_ids, attention_mask = self.tokenize(TEXTS[idxs].tolist())\n",
    "                # convert to torch tensors\n",
    "                input_ids = torch.tensor(input_ids).to(self.DEVICE)\n",
    "                attention_mask = torch.tensor(attention_mask).to(self.DEVICE)\n",
    "                labels = torch.tensor(GENERATED[idxs]).to(self.DEVICE)\n",
    "                aux_labels = torch.tensor(AUX_LABELS[idxs]).to(self.DEVICE)\n",
    "                # Shard Over TPU Nodes\n",
    "                xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "                xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "                xs.mark_sharding(labels, mesh, (0, 1))\n",
    "                xs.mark_sharding(aux_labels, mesh, (0, 1))\n",
    "                yield input_ids, attention_mask, labels, aux_labels\n",
    "    \n",
    "    # Save the trained model as output files\n",
    "    def save_model(self):\n",
    "        self.model = self.model.cpu()# Move model first on CPU before saving weights \n",
    "        self.model.base_model.save_pretrained(self.SAVE_PATH) # Save the entire fine-tuned model\n",
    "        self.tokenizer.save_pretrained(self.SAVE_PATH) # Save tokenizer for inference\n",
    "        # Save the LLM classification head\n",
    "        torch.save(self.model.llm_head.cpu().state_dict(), f\"{self.SAVE_PATH}/llm_head.pth\")     \n",
    "        # Save all the trainable parameters\n",
    "        torch.save(dict([(k,v) for k, v in self.model.named_parameters() if v.requires_grad]), f\"{self.SAVE_PATH}/model.pth\")\n",
    "        print(f\"Save the model and tokenizers to {self.SAVE_PATH}\")\n",
    "    \n",
    "      # Validate the model\n",
    "    def valid_model(self):\n",
    "        num_devices, mesh = self.partition_mesh()\n",
    "        # print(f'Number_DEVICES: {num_devices}')\n",
    "        \n",
    "        # Use the non-fold dataset as valid dataset\n",
    "        fold_valid_df = self.train_data[self.train_data[\"fold\"] == self.fold]\n",
    "        if self.DEBUG:\n",
    "            fold_valid_df = fold_valid_df.sample(n=100, random_state=self.SEED)  # Validate on small samples \n",
    "        \n",
    "        # Compute total samples and number of steps in one epochs\n",
    "        N_SAMPLES = len(fold_valid_df)\n",
    "        print(f\"Start validating the model with number of sample {N_SAMPLES}\")\n",
    "\n",
    "        # Generated By AI Label of Texts\n",
    "        TEXTS = fold_valid_df['text'].values\n",
    "        GENERATED = fold_valid_df['label'].values.reshape(-1,1).astype(np.float32)\n",
    "        AUX_LABELS = fold_valid_df['aux_label'].values.reshape(-1,1).astype(np.float32)\n",
    "        # Create a valid dataset \n",
    "        VALID_DATASET = self.create_dataset(TEXTS, GENERATED, AUX_LABELS, N_SAMPLES, mesh)\n",
    "\n",
    "        # Put Model In Eval Modus\n",
    "        self.model.eval()\n",
    "        # Loss Function\n",
    "        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n",
    "        METRICS = {'loss': [], \n",
    "                   'auc': {'y_true': [], 'y_pred': []} }\n",
    "        STEPS = N_SAMPLES // self.BATCH_SIZE\n",
    "        for step in tqdm(range(STEPS)):\n",
    "            # Enable inference mode using `no_grad`\n",
    "            with torch.no_grad():\n",
    "                start = time.time()\n",
    "                # Get Batch\n",
    "                input_ids, attention_mask, labels, _ = next(VALID_DATASET)\n",
    "                 # Forward Pass\n",
    "                outputs, _ = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                # Logits Float32\n",
    "                logits = outputs.to(dtype=torch.float32)\n",
    "                # Backward Pass\n",
    "                loss = LOSS_FN(logits, labels)\n",
    "                # Update Metrics And Progress Bar\n",
    "                METRICS['loss'].append(float(loss))\n",
    "                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n",
    "                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n",
    "\n",
    "        # Compute and display the validation results\n",
    "        print(f\"Number of validation data {len(fold_valid_df)}\\n\"\n",
    "              f\"loss (valid): {np.mean(METRICS['loss']): .3f}\\n\"\n",
    "              f\"auc (valid): {sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred']):.3f}\")\n",
    " \n",
    "    # Train the model by the fold data\n",
    "    def train_model(self):\n",
    "        num_devices, mesh = self.partition_mesh()\n",
    "        print(f'Number_DEVICES: {num_devices}')\n",
    "        # Use the fold data as training data\n",
    "        fold_train_df = self.train_data[self.train_data[\"fold\"] != self.fold]\n",
    "        if self.DEBUG:\n",
    "            fold_train_df = fold_train_df.sample(frac =.1, random_state=SEED) # Select a small amount of samples \n",
    "        # limited to two columns\n",
    "        fold_train_df = fold_train_df[['text', 'label', 'aux_label']]\n",
    "#         print(\"Start correcting the typos in training dataset\")\n",
    "#         start = time.time()\n",
    "#         # Preprocess the text to correct the typos \n",
    "#         fold_train_df['text'] = fold_train_df['text'].map(lambda text: pre_processing_text(text))\n",
    "#         print(f\"Complete correcting the texts {len(fold_train_df['text'])} in {time.time() - start : .1f}\")\n",
    "        # Compute total samples and number of steps in one epochs\n",
    "        N_SAMPLES = len(fold_train_df)\n",
    "        # Compute the total steps per epochs\n",
    "        STEPS_PER_EPOCH = N_SAMPLES // self.BATCH_SIZE\n",
    "        print(f'BATCH_SIZE: {self.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')\n",
    "        \n",
    "        # Generated By AI Label of Texts\n",
    "        TEXTS = fold_train_df['text'].values\n",
    "        GENERATED = fold_train_df['label'].values.reshape(-1,1).astype(np.float32)\n",
    "        AUX_LABELS = fold_train_df['aux_label'].values.reshape(-1,1).astype(np.float32)\n",
    "        print(f'TEXTS shape: {TEXTS.shape}\\n'\n",
    "              f'AUX_LABELS shape: {AUX_LABELS.shape}\\n'\n",
    "              f'GENERATED shape: {GENERATED.shape}')\n",
    "        \n",
    "        # Create a train dataset\n",
    "        TRAIN_DATASET = self.create_dataset(TEXTS, GENERATED, AUX_LABELS, N_SAMPLES, mesh)\n",
    "        \n",
    "        # Create optimizer and lr_scheduler\n",
    "        optimizer, lr_scheduler = self.create_optimizer_scheduler(STEPS_PER_EPOCH, self.NUM_EPOCHS)\n",
    "        \n",
    "        # Put Model In Train Modus\n",
    "        self.model.train()\n",
    "        # Loss Function, basic Binary Cross Entropy\n",
    "        LOSS_FN = torch.nn.BCEWithLogitsLoss().to(dtype=torch.float32)\n",
    "        LOSS_FN_AUX = torch.nn.CrossEntropyLoss(label_smoothing=0.1).to(dtype=torch.float32)\n",
    "        # Training loop goes through each epoch\n",
    "        for epoch in tqdm(range(self.NUM_EPOCHS)):\n",
    "            METRICS = {'loss': [], \n",
    "                       'auc': {'y_true': [], 'y_pred': []} }\n",
    "            # Go through each step\n",
    "            for step in range(STEPS_PER_EPOCH):\n",
    "                # Zero Out Gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Get Batch\n",
    "                input_ids, attention_mask, labels, aux_labels = next(TRAIN_DATASET)\n",
    "                # Test the TRAIN_DATASET for debugging first record\n",
    "                # Forward Pass\n",
    "                outputs, aux_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                # Logits Float32\n",
    "                logits = outputs.to(dtype=torch.float32)\n",
    "                aux_logits = aux_outputs.to(dtype=torch.float32)\n",
    "                # Backward Pass\n",
    "                if self.AUX_WEIGHT:\n",
    "                    loss = LOSS_FN(logits, labels)\n",
    "                    aux_loss = LOSS_FN_AUX(aux_logits, aux_labels.squeeze())\n",
    "                    loss = (loss * (1-self.AUX_WEIGHT)) + (aux_loss * self.AUX_WEIGHT)\n",
    "                    loss.backward()\n",
    "                else:\n",
    "                    loss = LOSS_FN(logits, labels)\n",
    "                    loss.backward()\n",
    "                # Update Weights\n",
    "                optimizer.step()\n",
    "                xm.mark_step()\n",
    "                # Update Learning Rate Scheduler\n",
    "                lr_scheduler.step()\n",
    "                # Update Metrics And Progress Bar\n",
    "                METRICS['loss'].append(float(loss))\n",
    "                METRICS['auc']['y_true'] += labels.squeeze().tolist()\n",
    "                METRICS['auc']['y_pred'] += logits.sigmoid().tolist()\n",
    "                # print(f\"Complete updating metrics {METRICS}\")\n",
    "                # Metrics Shown After Both Classes Present\n",
    "                if np.unique(METRICS['auc']['y_true']).size == 2:\n",
    "                    metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n",
    "                    metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n",
    "                    metrics += ', µ_auc: {:.3f}'.format(\n",
    "                        sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'], METRICS['auc']['y_pred'])\n",
    "                    )\n",
    "\n",
    "                    lr = optimizer.param_groups[0]['lr']\n",
    "                    print('\\r'*100, f'{epoch+1:02}/{self.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
    "            avg_loss = np.mean(METRICS['loss'])\n",
    "            roc_auc_score = sklearn.metrics.roc_auc_score(METRICS['auc']['y_true'],\n",
    "                                                          METRICS['auc']['y_pred'])\n",
    "            print(f'\\nFinish EPOCH {epoch} with average_loss: {avg_loss: .5f} '\n",
    "                  f'ROC_AUC_Score: {roc_auc_score:.5f}')\n",
    "            \n",
    "    # Clear the memory\n",
    "    def clear_memory(self):\n",
    "        del self.model, self.tokenizer\n",
    "        libc.malloc_trim(0)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1160db8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T05:36:32.181793Z",
     "iopub.status.busy": "2024-02-24T05:36:32.181573Z",
     "iopub.status.idle": "2024-02-24T09:15:44.374809Z",
     "shell.execute_reply": "2024-02-24T09:15:44.373646Z"
    },
    "papermill": {
     "duration": 13152.203849,
     "end_time": "2024-02-24T09:15:44.378459",
     "exception": false,
     "start_time": "2024-02-24T05:36:32.174610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "      <th>aux_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Check whether the following essay is human written or AI generated? Remember, the text might ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>train_essay</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "0  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "1  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "2  Check whether the following essay is human written or AI generated? Remember, the text might ha...   \n",
       "\n",
       "   label       source  fold  aux_label  \n",
       "0      0  train_essay   2.0          0  \n",
       "1      0  train_essay   2.0          0  \n",
       "2      0  train_essay   4.0          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.35s/it]\n",
      "/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "trainable params: 6,422,528 || all params: 8,544,103,424 || trainable%: 0.07516912754074827\n",
      "Complete loading pretrained LLM model\n",
      "Number_DEVICES: 8\n",
      "BATCH_SIZE: 16, N_SAMPLES: 50162, STEPS_PER_EPOCH: 3135\n",
      "TEXTS shape: (50162,)\n",
      "AUX_LABELS shape: (50162, 1)\n",
      "GENERATED shape: (50162, 1)\n",
      "Complete creating optimizer and lr scheduler\n",
      "optimizer AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0003\n",
      "    lr: 0.0\n",
      "    maximize: False\n",
      "    weight_decay: 2.9999999999999997e-05\n",
      ")\n",
      "lr_scheduler <torch.optim.lr_scheduler.LambdaLR object at 0x7e1f007b5db0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01/01 | 3134/3135 lr: 7.83E-11, µ_loss: 0.560, step_loss: 0.362, µ_auc: 0.999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [3:06:13<00:00, 11173.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01/01 | 3135/3135 lr: 0.00E+00, µ_loss: 0.560, step_loss: 0.359, µ_auc: 0.999\n",
      "Finish EPOCH 0 with average_loss:  0.55993 ROC_AUC_Score: 0.99904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validating the model with number of sample 12541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [30:12<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation data 12541\n",
      "loss (valid):  0.007\n",
      "auc (valid): 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/gemma/transformers/7b/2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the model and tokenizers to /kaggle/working/gemma7b_fold0_TPU\n"
     ]
    }
   ],
   "source": [
    "as_completed# Start training the model\n",
    "display(train_data.head(3))\n",
    "\n",
    "fold = 0\n",
    "epochs = 1\n",
    "trainer = TrainModelTPU(fold, epochs, train_data)\n",
    "\n",
    "# Train the model\n",
    "trainer.train_model()\n",
    "# Validate the model\n",
    "trainer.valid_model()\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model()\n",
    " \n",
    "# Clear the memorys\n",
    "trainer.clear_memory()\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29f5ad77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T09:15:45.251199Z",
     "iopub.status.busy": "2024-02-24T09:15:45.250593Z",
     "iopub.status.idle": "2024-02-24T09:15:45.255599Z",
     "shell.execute_reply": "2024-02-24T09:15:45.254775Z"
    },
    "papermill": {
     "duration": 0.471761,
     "end_time": "2024-02-24T09:15:45.257308",
     "exception": false,
     "start_time": "2024-02-24T09:15:44.785547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from prettytable import PrettyTable\n",
    "\n",
    "# def count_parameters(model):\n",
    "#     table = PrettyTable([\"Modules\", \"trainable\", \"Parameters\"])\n",
    "#     total_params = 0\n",
    "#     trainable_params = 0\n",
    "#     for name, parameter in model.named_parameters():\n",
    "#         trainable = True\n",
    "#         if not parameter.requires_grad:\n",
    "#             trainable = False\n",
    "#         params = parameter.numel()\n",
    "#         table.add_row([name, trainable, params])\n",
    "#         total_params += params\n",
    "#         if trainable:\n",
    "#             trainable_params += params\n",
    "#     print(table)\n",
    "#     print(f\"Total Params: {total_params}\")\n",
    "#     print(f\"Total Trainable Params: {trainable_params}, {trainable_params / total_params}% trainable\")\n",
    "#     return total_params\n",
    "    \n",
    "# count_parameters(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d17299a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T09:15:46.061315Z",
     "iopub.status.busy": "2024-02-24T09:15:46.060402Z",
     "iopub.status.idle": "2024-02-24T09:15:46.065002Z",
     "shell.execute_reply": "2024-02-24T09:15:46.064150Z"
    },
    "papermill": {
     "duration": 0.409011,
     "end_time": "2024-02-24T09:15:46.066725",
     "exception": false,
     "start_time": "2024-02-24T09:15:45.657714",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Start training the model\n",
    "# display(train_data.head(3))\n",
    "# try:\n",
    "#     fold = 0\n",
    "#     # train on fold-th data and epoch-th\n",
    "#     for epoch in list(range(1, 11)):\n",
    "#         trainer = TrainModelTPU(fold, epoch, train_data)\n",
    "#         trainer.train_model() # Train the model    \n",
    "#         trainer.valid_model() # Validate the model\n",
    "#         trainer.save_model() # Save model and tokenizer\n",
    "#         trainer.clear_memory()\n",
    "#         del trainer\n",
    "# except:\n",
    "#     print(\"Something is wrong during training\")\n",
    "#     sys.exit(-1) # Stop with error\n",
    "# print(f\"Complete training the model on fold {fold}\")\n",
    "# sys.exit(0) # Stop the notebook normally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2065c39e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T09:15:46.867088Z",
     "iopub.status.busy": "2024-02-24T09:15:46.866229Z",
     "iopub.status.idle": "2024-02-24T09:15:46.871244Z",
     "shell.execute_reply": "2024-02-24T09:15:46.870401Z"
    },
    "papermill": {
     "duration": 0.423987,
     "end_time": "2024-02-24T09:15:46.872971",
     "exception": false,
     "start_time": "2024-02-24T09:15:46.448984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     from __future__ import annotations\n",
    "#     import time, sys, gc, logging, random\n",
    "#     from pathlib import Path\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     from tqdm import tqdm\n",
    "#     from datasets import Dataset\n",
    "#     from sklearn.model_selection import StratifiedKFold\n",
    "#     from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig \n",
    "#     from peft import TaskType, prepare_model_for_kbit_training, AutoPeftModelForCausalLM # type: ignore\n",
    "#     from transformers import BitsAndBytesConfig\n",
    "#     import torch\n",
    "\n",
    "#     from transformers import logging as hf_logging\n",
    "#     from transformers import (\n",
    "#         AutoTokenizer, AutoModelForSequenceClassification, LlamaForSequenceClassification,\n",
    "#         TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "#     import transformers\n",
    "\n",
    "#     import peft\n",
    "#     from accelerate import Accelerator\n",
    "#     import bitsandbytes\n",
    "#     from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "#     from shutil import rmtree\n",
    "#     import language_tool_python\n",
    "#     import optuna\n",
    "#     import concurrent\n",
    "#     from concurrent.futures import ThreadPoolExecutor\n",
    "#     from concurrent.futures import wait\n",
    "\n",
    "#     print(transformers.__version__)\n",
    "#     print(peft.__version__)\n",
    "#     print(torch.__version__)\n",
    "\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece51561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T09:15:47.660007Z",
     "iopub.status.busy": "2024-02-24T09:15:47.659470Z",
     "iopub.status.idle": "2024-02-24T09:15:47.669899Z",
     "shell.execute_reply": "2024-02-24T09:15:47.669071Z"
    },
    "papermill": {
     "duration": 0.415146,
     "end_time": "2024-02-24T09:15:47.671716",
     "exception": false,
     "start_time": "2024-02-24T09:15:47.256570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class TrainModelGPU:\n",
    "#     def __init__(self, fold):\n",
    "#         self.fold = fold\n",
    "#         self.TARGET_MODEL = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "#         self.load_model()\n",
    "\n",
    "#     # Load the pretrained model and add an extra layer with PEFT library for fine-tuning\n",
    "#     def load_model(self):\n",
    "#         # Enable GPU to run the model with 4bit\n",
    "#         bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "#         # LoRA: Low-Rank Adaptation of Large Language Models is a popular approach to fine-tune LLM on a single GPU\n",
    "#         # https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-2-d8e23877ac6f\n",
    "#         lora_config = LoraConfig(            \n",
    "#             r=16, # A larger 'r' value needs to update more parameters \n",
    "#             lora_dropout=0.10,\n",
    "#             bias='none',\n",
    "#             task_type=TaskType.SEQ_CLS,\n",
    "#             inference_mode=False,\n",
    "#             #Only targeting attention blocks of the model or targeting all linear layers\n",
    "#             target_modules=[\"q_proj\", \"v_proj\"] # if DEBUG else ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head'],                \n",
    "#         )\n",
    "#         # Load the tokenizer\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL, use_fast=False)\n",
    "#         self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "#         base_model = LlamaForSequenceClassification.from_pretrained(TARGET_MODEL,\n",
    "#                                                                     num_labels=2, # label is 0 or 1\n",
    "#                                                                     quantization_config=bnb_config,                                                                 \n",
    "#                                                                     device_map=\"auto\")\n",
    "\n",
    "#         base_model.config.pretraining_tp = 1 # 1 is 7b\n",
    "#         base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        \n",
    "#         # Load the model\n",
    "#         start = time.time()    \n",
    "#         # Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of \n",
    "#         # pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. \n",
    "#         # training_model = prepare_model_for_kbit_training(base_model)\n",
    "#         model = get_peft_model(base_model, lora_config)\n",
    "#         print(f\"Complete loading the base model {time.time() - start: .1f} seconds\") \n",
    "#         model.print_trainable_parameters() # Display the trainable parameters\n",
    "        \n",
    "#         return model, tokenizer\n",
    "    \n",
    "#     def preprocess_function(self, examples, tokenizer, max_length=512):\n",
    "#         examples[\"text\"] = list(map(lambda text: self.pre_processing_text(text), examples[\"text\"]))\n",
    "#         return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=True)\n",
    "\n",
    "#     def compute_metrics(self, eval_pred):\n",
    "#         predictions, labels = eval_pred\n",
    "#         predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "#         accuracy_val = accuracy_score(labels, predictions)\n",
    "#         roc_auc_val = roc_auc_score(labels, predictions)\n",
    "#         r = { \"accuracy\": accuracy_val,\n",
    "#               \"roc_auc\": roc_auc_val}\n",
    "#         # logging.debug(f'{r}')\n",
    "#         return r\n",
    "\n",
    "#     # Fine Tuning LLM: Parameter Efficient Fine Tuning (PEFT) and Lora configurations\n",
    "#     # https://github.com/microsoft/LoRA\n",
    "#     def train_model_by_fold(self, fold):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "#         print(f\"Start training the fold {fold} model\")\n",
    "#         # Create train and valid dataset for a fold\n",
    "#         fold_valid_df = train_data[train_data[\"fold\"] == fold]\n",
    "#         fold_train_df = train_data[train_data[\"fold\"] != fold]\n",
    "#         # Train the model with small (for debugging) or large samples\n",
    "#         if DEBUG:\n",
    "#             fold_train_df = fold_train_df.sample(frac =.1, random_state=SEED)\n",
    "#             fold_valid_df = fold_valid_df.sample(frac =.3, random_state=SEED)\n",
    "#         else:\n",
    "#             fold_train_df = fold_train_df.sample(frac =.3, random_state=SEED)\n",
    "#             fold_valid_df = fold_valid_df.sample(frac =1.0, random_state=SEED)\n",
    "\n",
    "#         print(f'fold_train_df: Len = {len(fold_train_df)} Counts = {fold_train_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
    "#         print(f'fold_valid_df: Len = {len(fold_valid_df)} Counts = {fold_valid_df.groupby(\"fold\")[\"label\"].value_counts()}')\n",
    "#         # create the dataset\n",
    "#         train_ds = Dataset.from_pandas(fold_train_df)\n",
    "#         valid_ds = Dataset.from_pandas(fold_valid_df)\n",
    "\n",
    "#         # Load the pretrained model and tokenizer\n",
    "#         model, tokenizer = load_model(fold)\n",
    "\n",
    "#         # Tokenize the train and valid dataset and pass tokenizer as function argument\n",
    "#         train_tokenized_ds = train_ds.map(self.preprocess_function, batched=True,\n",
    "#                                           fn_kwargs={\"tokenizer\": tokenizer})\n",
    "#         valid_tokenized_ds = valid_ds.map(self.preprocess_function, batched=True,\n",
    "#                                           fn_kwargs={\"tokenizer\": tokenizer})\n",
    "#         # Create data collator with padding (padding to the longest sequence)\n",
    "#         data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "\n",
    "#         # Start training processing        \n",
    "#         TMP_DIR = Path(f\"/kaggle/tmp/mistral_7b_fold{fold}/\")\n",
    "#         TMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "#         STEPS = 5 if DEBUG else 20\n",
    "#         EPOCHS = 1 if DEBUG else 10\n",
    "#         LEARNING_RATE = 1e-4 \n",
    "#         training_args = TrainingArguments(output_dir=TMP_DIR,\n",
    "#                                           overwrite_output_dir=True,\n",
    "#                                           fp16=True, #converts to float precision 16 using bitsandbytes\n",
    "#                                           learning_rate=LEARNING_RATE,                                      \n",
    "#                                           per_device_train_batch_size=1,\n",
    "#                                           per_device_eval_batch_size=1,\n",
    "#                                           gradient_accumulation_steps=16,\n",
    "#                                           max_grad_norm=0.3,\n",
    "#                                           optim='paged_adamw_32bit',\n",
    "#                                           lr_scheduler_type=\"cosine\",\n",
    "#                                           num_train_epochs=EPOCHS,\n",
    "#                                           weight_decay=0.01,\n",
    "#                                           evaluation_strategy=\"epoch\",\n",
    "#                                           save_strategy=\"epoch\",\n",
    "#                                           load_best_model_at_end=True,\n",
    "#                                           metric_for_best_model=\"roc_auc\",\n",
    "#                                           label_names=[\"label\"],\n",
    "#                                           push_to_hub=False,\n",
    "#                                           warmup_steps=STEPS,\n",
    "#                                           eval_steps=STEPS,\n",
    "#                                           logging_steps=STEPS,\n",
    "#                                           report_to='none', # if DEBUG else 'wandb'\n",
    "#                                           log_level='warning', # 'warning' is default level \n",
    "#                                          )\n",
    "#         start = time.time()\n",
    "#         # Create the trainer \n",
    "#         trainer = Trainer(model=model,\n",
    "#                           args=training_args,\n",
    "#                           train_dataset=train_tokenized_ds,\n",
    "#                           eval_dataset=valid_tokenized_ds,\n",
    "#                           tokenizer=tokenizer,\n",
    "#                           data_collator=data_collator,\n",
    "#                           compute_metrics=self.compute_metrics)\n",
    "\n",
    "#         trainer.train()\n",
    "\n",
    "#         OUTPUT_DIR = Path(f\"/kaggle/working/mistral_7b_fold{fold}/\")\n",
    "#         OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "#         # Save the full model and the training arguments\n",
    "#         trainer.save_model(str(OUTPUT_DIR))\n",
    "#         print(f\"=== Finish the training for fold {fold} in {time.time() - start:.1f} seconds ===\")\n",
    "#         del model, trainer, tokenizer\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 1825054,
     "sourceId": 2977194,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3972872,
     "sourceId": 6921012,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4309752,
     "sourceId": 7409832,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 6206,
     "sourceId": 11413,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30581,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13320.879213,
   "end_time": "2024-02-24T09:15:52.256500",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-24T05:33:51.377287",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
